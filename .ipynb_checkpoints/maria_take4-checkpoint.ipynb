{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maria/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# import xgboost\n",
    "from sklearn import cross_validation, tree, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "# tanvi's stuff\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as k_back\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, skiprows = 1):\n",
    "    '''   \n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\\n\",\n",
    "\n",
    "    Inputs:\n",
    "    filename: given as a string.\n",
    "    Outputs:\\n\",\n",
    "    Data contained in the file, returned as a numpy ndarray\\n\",\n",
    "    '''\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load in data once!\n",
    "train = load_data(\"data/train_2008.csv\")\n",
    "test = load_data(\"data/test_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIYAN'S XGDBOOST CODE\n",
    "\n",
    "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "\n",
    "y_train_new = train[:,train.shape[1]-1:]\n",
    "X_train = train[:,:train.shape[1]-1]\n",
    "    \n",
    "X_test = test[:,3:]\n",
    "y_train_new = train[:,train.shape[1]-1:]\n",
    "X_train = train[:,:train.shape[1]-1]\n",
    "X_train = X_train[:,3:]\n",
    "\n",
    "xgb.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TANVI'S NEURAL NET CODE\n",
    "\n",
    "x_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "x_test = test[:,:]\n",
    "\n",
    "merged = np.concatenate((x_train, x_test), axis=0)\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "merged = mmscaler.fit_transform(merged)\n",
    "\n",
    "# define base model\n",
    "def easy_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "easy_estimator = KerasRegressor(build_fn=easy_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# perform cross validation\n",
    "easy_kfold = KFold(n_splits=10)\n",
    "easy_results = cross_val_score(easy_estimator, x_train, y_train, cv=easy_kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (easy_results.mean(), easy_results.std()))\n",
    "\n",
    "# fit to train, predict both train and test\n",
    "easy_estimator.fit(x_train, y_train)\n",
    "easy_y_train = easy_estimator.predict(x_train)\n",
    "easy_y_test = easy_estimator.predict(x_test)\n",
    "\n",
    "# reshape to (x, 1) and normalize to (0,1)\n",
    "norm_y_train = easy_y_train.reshape(64667, 1)\n",
    "norm_y_test = easy_y_test.reshape(16000, 1)\n",
    "\n",
    "# normalize output data\n",
    "norm_y_train = mmscaler.fit_transform(norm_y_train)\n",
    "norm_y_test = mmscaler.fit_transform(norm_y_test)\n",
    "\n",
    "roc_auc_score(y_train, norm_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73242089888730089"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MARIA'S RANDOM FOREST CODE\n",
    "\n",
    "X_train = (np.asarray(train))[:, 3:382].astype(np.float)\n",
    "y_train = (np.asarray(train))[:, 382].astype(np.float)\n",
    "X_test = (np.asarray(test))[:, 3:382].astype(np.float)\n",
    "y_test = (np.asarray(test))[:, 381].astype(np.float)\n",
    "\n",
    "rf = RandomForestRegressor(max_features=2, min_samples_split=4, n_estimators=50, min_samples_leaf=2, max_depth=8)\n",
    "\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = rf_model.predict(X_train)\n",
    "\n",
    "roc_auc_score(y_train, y_predict)\n",
    "\n",
    "y_predict_test = rf_model.predict(X_test)\n",
    "y_predict_test = y_predict_test.reshape(16000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIVING UP FOR NOW AND JUST IMPORTING THEM FROM THE CSV\n",
    "\n",
    "import csv\n",
    "nn = []\n",
    "with open('nn.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        nn.append(row[1])\n",
    "\n",
    "xgb = []\n",
    "with open('xg.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        xgb.append(row[1])\n",
    "        \n",
    "rf = []\n",
    "with open('rf.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        rf.append(row[1])\n",
    "        \n",
    "dt = []\n",
    "with open('dt.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        dt.append(row[1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_prediction = np.array(nn[1:])\n",
    "nn_prediction = nn_prediction.astype(float)\n",
    "nn_prediction = np.reshape(nn_prediction, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_prediction = np.array(xgb[1:])\n",
    "xgb_prediction = xgb_prediction.astype(float)\n",
    "xgb_prediction = np.reshape(xgb_prediction, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_prediction = np.array(rf[1:])\n",
    "rf_prediction = rf_prediction.astype(float)\n",
    "rf_prediction = np.reshape(rf_prediction, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_prediction = np.array(rf[1:])\n",
    "dt_prediction = dt_prediction.astype(float)\n",
    "dt_prediction = np.reshape(dt_prediction, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = (nn_prediction + xgb_prediction + rf_prediction + dt_prediction) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmscaler = MinMaxScaler()\n",
    "merged_norm = mmscaler.fit_transform(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test[:,:1]\n",
    "ids2 = ids.astype(int)\n",
    "out = np.concatenate((ids2, merged_norm), axis=1)\n",
    "np.savetxt('merged6.csv', out, delimiter=',', fmt='%1.10f', header = 'id,target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5512816 ],\n",
       "       [ 0.1873098 ],\n",
       "       [ 0.2175072 ],\n",
       "       ..., \n",
       "       [ 0.31012226],\n",
       "       [ 0.15215982],\n",
       "       [ 0.37198378]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.asarray(out)\n",
    "for i in range(len(out)):\n",
    "    out[i] = (out[i][0].astype(int), out[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   5.06451289e-01],\n",
       "       [  1.00000000e+00,   1.41258482e-01],\n",
       "       [  2.00000000e+00,   1.71557180e-01],\n",
       "       ..., \n",
       "       [  1.59970000e+04,   2.64482938e-01],\n",
       "       [  1.59980000e+04,   1.05990588e-01],\n",
       "       [  1.59990000e+04,   3.26551980e-01]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[4][0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35504984417457297"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
